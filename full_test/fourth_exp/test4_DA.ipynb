{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = 'prepared_facebook_data'\n",
    "# folder_name = 'facebook_comm_vol'\n",
    "dataset_name ='house_prices'\n",
    "folder_name = 'boston_house_prices'\n",
    "# dataset_name = 'energy_consumption'\n",
    "# folder_name = 'energy_consumption'\n",
    "# dataset_name = 'physics'\n",
    "# folder_name = 'physics'\n",
    "# dataset_name = 'quote'\n",
    "# folder_name = 'quote'\n",
    "\n",
    "path = f\"../../other_data_experiments/{folder_name}/\"\n",
    "fourth_test_path = path + \"fourth experiment/prepared/\"\n",
    "third_test_path = path + \"third experiment/\"\n",
    "GRAPHS_DIR = path + \"graphs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_file = path + f\"first experiment/res_grid_search_{dataset_name}.csv\"\n",
    "\n",
    "# bayes_opt_files = [third_test_path + f\"bayes_opt_{i}_{dataset_name}.csv\"\n",
    "#                        for i in range(1,11)]\n",
    "bayes_opt_files = [third_test_path + f\"bayes_opt_{i}.csv\"\n",
    "                       for i in range(1,11)]\n",
    "\n",
    "random_search_files = [f\"first experiment/res_rand_{dataset_name}_{i}.csv\"\n",
    "                       for i in range(1,11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_df = pd.read_csv(grid_file, usecols = ['experiment_name',\n",
    "                                           'mean_test_score',\n",
    "                                           'params'])\n",
    "grid_df['run_number']=1\n",
    "grid_df['iter_num'] = range(1, grid_df.shape[0]+1)    \n",
    "\n",
    "default_df = pd.read_csv(path + f\"first experiment/res_default_{dataset_name}.csv\", \n",
    "                         usecols = ['experiment_name',\n",
    "                                    'mean_test_score',\n",
    "                                    'params'])\n",
    "default_df['run_number']=1\n",
    "default_df['iter_num'] = range(1, default_df.shape[0]+1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_df = pd.DataFrame([])\n",
    "for file_name in bayes_opt_files:\n",
    "    file_df = pd.read_csv(file_name, \n",
    "                          usecols = ['experiment_name', 'mean_test_score',\n",
    "                                     'params','run_number'])\n",
    "    file_df['iter_num'] = range(1, file_df.shape[0]+1) \n",
    "    bayes_df = pd.concat([bayes_df, file_df], sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters_info = {}\n",
    "scores_info = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set stop criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "stop_iter_num = 10\n",
    "# 10, 25, 50 #, 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_group_files_with_stop = [fourth_test_path + \\\n",
    "                               f\"GRBO_groups_with_{stop_iter_num}_{i}_{dataset_name}.csv\"\n",
    "                               for i in range(0, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_bayes_group_with_stop = pd.DataFrame([])\n",
    "\n",
    "for file_name in bayes_group_files_with_stop:\n",
    "    file_df = pd.read_csv(file_name, \n",
    "                         usecols = ['experiment_name', 'mean_test_score',\n",
    "                                     'params','run_number'])\n",
    "    file_df['iter_num'] = range(1, file_df.shape[0]+1) \n",
    "    full_bayes_group_with_stop = pd.concat([full_bayes_group_with_stop, file_df], sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = full_bayes_group_with_stop.groupby('run_number')['mean_test_score'].max()\n",
    "print(\"MEAN best score:\", np.mean(scores))\n",
    "print(\"STD best score:\", np.std(scores))\n",
    "print(f\"{np.round(np.mean(scores), 4)} _ {np.round(np.std(scores), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters_num = full_bayes_group_with_stop.groupby(by='run_number')['iter_num'].max()\n",
    "print(\"MEAN iters num:\", np.mean(iters_num))\n",
    "# print(\"STD iters num:\", np.std(iters_num))\n",
    "max_iter = np.round(np.max(iters_num))\n",
    "min_iter = np.round(np.min(iters_num))\n",
    "print(\"MAX iters num:\", max_iter)\n",
    "print(\"MIN iters num:\", min_iter)\n",
    "print(f\"min: {int(min_iter)}\\nmean: {int(np.round(np.mean(iters_num)))}\\nmax: {int(max_iter)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score_and_iter = full_bayes_group_with_stop.groupby(by = 'run_number')[['mean_test_score', 'iter_num']].max()\n",
    "\n",
    "full_bayes_group_with_stop = full_bayes_group_with_stop.loc[:, ['mean_test_score', 'iter_num', 'run_number']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Продление конечной оценки до максимальной итерации \n",
    "# для отрисовки конца доверительного интервала"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_for_extension = list(max_score_and_iter.loc[\n",
    "        max_score_and_iter.iter_num < max_iter].index)\n",
    "\n",
    "full_bayes_group_with_stop = full_bayes_group_with_stop.dropna(how = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df_GRS = full_bayes_group_with_stop.groupby('run_number')['iter_num'].max().values\n",
    "\n",
    "iters_info['GRBO_{}'.format(stop_iter_num)] = {\n",
    "    'mean':np.mean(agg_df_GRS), \n",
    "    'std':np.nanstd(agg_df_GRS)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run_number in list_for_extension:\n",
    "    new_part_df = pd.DataFrame([], columns=full_bayes_group_with_stop.columns)\n",
    "    new_part_df['iter_num']=range(int(max_score_and_iter.loc[run_number, 'iter_num']+1), \n",
    "                                  int(max_iter)+1)\n",
    "    new_part_df['run_number'] = run_number\n",
    "    new_part_df['mean_test_score'] = max_score_and_iter.loc[run_number, \n",
    "                                                            'mean_test_score']\n",
    "    full_bayes_group_with_stop = pd.concat([full_bayes_group_with_stop, new_part_df], sort = False)\n",
    "    \n",
    "stopped_full_df = pd.DataFrame([])\n",
    "for run_number in np.unique(full_bayes_group_with_stop.run_number):\n",
    "    copy_df = full_bayes_group_with_stop.loc[full_bayes_group_with_stop.run_number == run_number].sort_values(by = 'iter_num')\n",
    "    copy_df['mean_test_score']=copy_df['mean_test_score'].cummax()\n",
    "    copy_df['run_number']=run_number\n",
    "    #full_df.loc[full_df.run_number == run_number]=full_df.loc[full_df.run_number == run_number].sort_values(by = 'iter_num')\n",
    "    #full_df.loc[full_df.run_number == run_number, 'mean_test_score'] = \\\n",
    "    #    full_df.loc[full_df.run_number == run_number, 'mean_test_score'].cummax()\n",
    "    stopped_full_df = pd.concat([stopped_full_df, copy_df], sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_bayes_group_with_stop = stopped_full_df\n",
    "full_bayes_group_with_stop['experiment_name']='group bayes opt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BAYES OPT\n",
    "#get best scores per itration for each bayes_opt performance:\n",
    "bo_df_stopped=pd.DataFrame([])\n",
    "max_iters_for_BO = []\n",
    "for i in bayes_df.run_number.unique():    \n",
    "    tmp=pd.DataFrame(bayes_df.loc[bayes_df.run_number==i, \n",
    "                                   \"mean_test_score\"].copy().cummax())\n",
    "    tmp[\"iter_num\"]=bayes_df.loc[bayes_df.run_number==i, \"iter_num\"]\n",
    "    curr_max=tmp.groupby(\"mean_test_score\").count().reset_index()\n",
    "    max_score=curr_max.loc[curr_max[\"iter_num\"]>=stop_iter_num, \n",
    "                           \"mean_test_score\"].values[0]\n",
    "    \n",
    "    tmp_before=tmp.loc[tmp[\"mean_test_score\"]<max_score] #get steps before stop criterion\n",
    "    tmp_stop=tmp.loc[tmp[\"mean_test_score\"]==max_score].sort_values(by=\"iter_num\")\n",
    "    tmp_stop=tmp_stop.iloc[:stop_iter_num]\n",
    "    df_stopped=pd.concat([tmp_before, tmp_stop], sort = False)  \n",
    "    df_stopped['run_number']=i\n",
    "    max_iters_for_BO.append(df_stopped.shape[0])\n",
    "    bo_df_stopped=pd.concat([bo_df_stopped, df_stopped], sort=False)\n",
    "    \n",
    "    \n",
    "print(\"MEAN iters num:\", np.mean(max_iters_for_BO))\n",
    "print(\"STD iters num:\", np.std(max_iters_for_BO))\n",
    "print(\"MAX iters num:\", np.max(max_iters_for_BO))\n",
    "\n",
    "print(f\"min: {int(np.round(np.min(max_iters_for_BO)))}\\nmean: {int(np.round(np.mean(max_iters_for_BO)))}\\nmax: {int(np.round(np.max(max_iters_for_BO)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters_info['BO_{}'.format(stop_iter_num)] = {\n",
    "    'mean':np.mean(max_iters_for_BO), \n",
    "    'std':np.nanstd(max_iters_for_BO)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_iter = bo_df_stopped.groupby(by='run_number')['iter_num'].max().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = np.max(max_iters_for_BO)\n",
    "max_score_and_iter_BO = bo_df_stopped.groupby(by = 'run_number')['mean_test_score', 'iter_num'].max()\n",
    "for run_number in list(max_score_and_iter_BO.loc[\n",
    "        max_score_and_iter_BO.iter_num < max_iter].index):\n",
    "    new_part_df = pd.DataFrame([], columns=bo_df_stopped.columns)\n",
    "    new_part_df['iter_num']=range(int(max_score_and_iter_BO.loc[run_number, 'iter_num']+1), \n",
    "                                  int(max_iter)+1)\n",
    "    new_part_df[['mean_test_score', 'run_number']] = max_score_and_iter_BO.loc[run_number, 'mean_test_score'], run_number\n",
    "    bo_df_stopped = pd.concat([bo_df_stopped, new_part_df])\n",
    "bo_df_stopped['experiment_name']='bayes opt'   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_df_stopped.mean_test_score = bo_df_stopped.mean_test_score.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data with stop criterion\n",
    "# stop if the cumulative score hasn't grown in \"stop_iter_num\" iterations\n",
    "# GRID_SEARCH\n",
    "#get best scores per itration for grid_search performance:\n",
    "grid_perf=grid_df.loc[grid_df['experiment_name']!='default params',\n",
    "                      'mean_test_score'].cummax()\n",
    "\n",
    "\n",
    "## BAYES OPT\n",
    "# get best scores per itration for each bayes_opt performance:\n",
    "# bo_df_stopped\n",
    "    \n",
    "#get mean and confidence interval value for random_search performance on iteration:    \n",
    "\n",
    "#get mean and confidence interval value for random_search performance on iteration:    \n",
    "z = 2.262 # 95% student for n = 10\n",
    "# z = 1.833 # 90% student for n = 10\n",
    "# if normal or n>=30:\n",
    "# z = 1.96 # for 95% conf interval (coefficient)\n",
    "# z = 1.64 # for 90% conf interval\n",
    "\n",
    "n=10\n",
    "bo_std=bo_df_stopped.groupby(by=\"iter_num\")['mean_test_score'].std() #std\n",
    "bo_std=(bo_std*z)/np.sqrt(n) #confidence interval value \n",
    "bo_mean=bo_df_stopped.groupby(by=\"iter_num\")['mean_test_score'].mean() #mean\n",
    "\n",
    "\n",
    "## GROUP_BAYES_OPT\n",
    "#get best scores per itration for each group_bayes_opt performance (same computations):\n",
    "\n",
    "n=10\n",
    "group_std=full_bayes_group_with_stop.groupby(by=\"iter_num\")['mean_test_score'].std()\n",
    "group_std=(group_std*z)/np.sqrt(n) \n",
    "group_std=group_std.fillna(0)\n",
    "group_mean=full_bayes_group_with_stop.groupby(by=\"iter_num\")['mean_test_score'].mean()\n",
    "\n",
    "## DEFAULT performance\n",
    "default_perf=default_df.loc[:,'mean_test_score'].values[0]\n",
    "\n",
    "\n",
    "## PLOT\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.grid()\n",
    "\n",
    "#grid plot:\n",
    "x=list(range(0, grid_perf.shape[0]))\n",
    "plt.plot(x, grid_perf, label=\"Поиск по сетке\")\n",
    "\n",
    "#conf for bayes opt:\n",
    "x=list(range(0, bo_mean.shape[0]))\n",
    "plt.plot(x, bo_mean, color=\"y\", label=\"Байесовская оптимизация без разбиения на группы\")\n",
    "lower_bo=bo_mean - bo_std\n",
    "upper_bo=bo_mean + bo_std\n",
    "plt.plot(x, lower_bo, alpha=0.7, color=\"y\")\n",
    "plt.plot(x, upper_bo, alpha=0.7, color=\"y\")\n",
    "plt.fill_between(x, lower_bo, upper_bo, alpha=0.1, color=\"y\") \n",
    "\n",
    "#conf for group bayes opt:\n",
    "x=list(range(0, group_mean.shape[0]))\n",
    "plt.plot(x, group_mean, color=\"green\", label=\"Байесовская оптимизация с разбиением на группы\")\n",
    "lower_group=group_mean - group_std\n",
    "upper_group=group_mean + group_std\n",
    "plt.plot(x, lower_group, alpha=0.5, color=\"green\")\n",
    "plt.plot(x, upper_group, alpha=0.5, color=\"green\")\n",
    "plt.fill_between(x, lower_group, upper_group, alpha=0.1, color=\"green\")\n",
    "\n",
    "#line for default values\n",
    "plt.axhline(default_perf, label=\"Параметры по умолчанию\", color='r')\n",
    "plt.axvline(243, label=\"Границы групп\", color='k', ls = \"--\")\n",
    "plt.axvline(243*2, color='k', ls = \"--\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(f\"Кривые обучения для эксперимента с критерием остановки \\nравному {stop_iter_num} итерациям\")\n",
    "plt.ylabel(\"Оценка R-квадрат\")\n",
    "plt.xlabel(\"Номер итерации\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(GRAPHS_DIR+f\"BO_scores_with_stop_{stop_iter_num} {dataset_name}.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df_GRBO = full_bayes_group_with_stop.groupby('run_number')['mean_test_score'].max().values\n",
    "agg_df_BO = bo_df_stopped.groupby('run_number')['mean_test_score'].max().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_info[f'GRBO_{stop_iter_num}'] = {\n",
    "    'mean':np.mean(agg_df_GRBO), \n",
    "    'std':np.nanstd(agg_df_GRBO)\n",
    "}\n",
    "scores_info[f'BO_{stop_iter_num}'] = {\n",
    "    'mean':np.mean(agg_df_BO), \n",
    "    'std':np.nanstd(agg_df_BO)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{np.round(np.mean(agg_df_BO), 4)} _ {np.round(np.nanstd(agg_df_BO), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get mean and confidence interval value for random_search performance on iteration:    \n",
    "z = 2.262 # 95% student for n = 10\n",
    "# z = 1.833 # 90% student for n = 10\n",
    "# if normal or n>=30:\n",
    "# z = 1.96 # for 95% conf interval (coefficient)\n",
    "# z = 1.64 # for 90% conf interval\n",
    "n=10\n",
    "for k in ['GRBO', 'BO']:\n",
    "    random_std=(scores_info[f'{k}_{stop_iter_num}']['std']*z)/np.sqrt(n) #confidence interval value \n",
    "    random_mean=scores_info[f'{k}_{stop_iter_num}']['mean']\n",
    "    print(f'{k}:\\n{np.round(random_mean-random_std, 4)} _ {np.round(random_mean+random_std, 4)}')\n",
    "    \n",
    "#     random_std=(iters_info[f'{k}_10']['std']*z)/np.sqrt(n) #confidence interval value \n",
    "#     random_mean=iters_info[f'{k}_10']['mean']\n",
    "#     print(f'{k}:\\n{int(np.round(random_mean-random_std))} _ {int(np.round(random_mean+random_std))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(path + \"fourth experiment/BO_scores_info.txt\", 'w') as outfile:\n",
    "#     json.dump(scores_info, outfile)\n",
    "    \n",
    "\n",
    "# with open(path + \"fourth experiment/BO_iters_info.txt\", 'w') as outfile:\n",
    "#     json.dump(iters_info, outfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
