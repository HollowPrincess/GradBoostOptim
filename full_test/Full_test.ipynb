{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Full_test",
      "provenance": [],
      "collapsed_sections": [
        "aCiPu-O1_K3x"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_x8zo0i-n4_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EXPERIMENTS STATES:\n",
        "has_default = True\n",
        "has_grid = True\n",
        "\n",
        "random_search_amount = 10\n",
        "group_random_search_amount = 10\n",
        "\n",
        "stop_criterion = 100\n",
        "first_group_stop_params = {1: {'max_depth': 3, 'min_child_weight': 3},\n",
        " 2: {'max_depth': 3, 'min_child_weight': 3},\n",
        " 3: {'max_depth': 3, 'min_child_weight': 3},\n",
        " 4: {'max_depth': 3, 'min_child_weight': 3},\n",
        " 5: {'max_depth': 3, 'min_child_weight': 3},\n",
        " 6: {'max_depth': 3, 'min_child_weight': 3},\n",
        " 7: {'max_depth': 3, 'min_child_weight': 5},\n",
        " 8: {'max_depth': 3, 'min_child_weight': 3},\n",
        " 9: {'max_depth': 3, 'min_child_weight': 3},\n",
        " 10: {'max_depth': 3, 'min_child_weight': 4}}\n",
        "first_group_stop_iters = {1: 243,\n",
        " 2: 211,\n",
        " 3: 203,\n",
        " 4: 243,\n",
        " 5: 159,\n",
        " 6: 243,\n",
        " 7: 132,\n",
        " 8: 184,\n",
        " 9: 243,\n",
        " 10: 243}\n",
        "# bayes_search = 0\n",
        "# bayes_group = 0\n",
        "\n",
        "dataset_name = 'house_prices'\n",
        "#download_dataset_name_id = 'BNG(credit-g)'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_Zwave5_oDY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#imports:\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import xgboost\n",
        "\n",
        "from scipy.stats import randint as sp_randint # int distribution for random search\n",
        "from scipy.stats import uniform # float distribution for random search\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPeb0WaaCfyA",
        "colab_type": "code",
        "outputId": "ac01c86c-1caa-4ef1-99e2-36e3a5b6c11e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCIt5LakCwVq",
        "colab_type": "code",
        "outputId": "842a20f9-553e-4e19-e186-b233fb56846d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/mntDrive') \n",
        "DIR_TO_SAVE_RES = \"/mntDrive/My Drive/GrBoost/Практика/test_full/data/{}/\".format(dataset_name)\n",
        "DIR_TO_DOWNLOAD_DATA = \"/mntDrive/My Drive/GrBoost/Практика/test_full/data/input/{}/\".format(dataset_name)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /mntDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FCFs3Ly_CJf",
        "colab_type": "code",
        "outputId": "b21fc6e3-6a8d-40b0-fb97-8858b6ea385b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# IMPORT DATA\n",
        "\"\"\"\n",
        "import os\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Fetch a dataset and create a local copy.\n",
        "if os.path.exists(DIR_TO_DOWNLOAD_DATA+'{}.npz'.format(dataset_name)):\n",
        "    with np.load(DIR_TO_DOWNLOAD_DATA+'{}.npz'.format(dataset_name), 'r', allow_pickle=True) as data:\n",
        "        X = data['X']\n",
        "        y = data['y']\n",
        "else:\n",
        "    dataset = fetch_openml(download_dataset_name_id, version=1)\n",
        "    X, y = dataset.data, dataset.target\n",
        "    np.savez(DIR_TO_DOWNLOAD_DATA+'{}.npz'.format(dataset_name), X=X, y=y)\n",
        "\n",
        "dtrain = xgboost.DMatrix(X, label=y)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nimport os\\nfrom sklearn.datasets import fetch_openml\\n\\n# Fetch a dataset and create a local copy.\\nif os.path.exists(DIR_TO_DOWNLOAD_DATA+'{}.npz'.format(dataset_name)):\\n    with np.load(DIR_TO_DOWNLOAD_DATA+'{}.npz'.format(dataset_name), 'r', allow_pickle=True) as data:\\n        X = data['X']\\n        y = data['y']\\nelse:\\n    dataset = fetch_openml(download_dataset_name_id, version=1)\\n    X, y = dataset.data, dataset.target\\n    np.savez(DIR_TO_DOWNLOAD_DATA+'{}.npz'.format(dataset_name), X=X, y=y)\\n\\ndtrain = xgboost.DMatrix(X, label=y)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_7paBP_dJTs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "X, y = load_boston(return_X_y=True)\n",
        "dtrain = xgboost.DMatrix(X, label=y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCiPu-O1_K3x",
        "colab_type": "text"
      },
      "source": [
        "# FIRST TEST: RANDOM AND GROUP RANDOM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAJJg_3q_mI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# default\n",
        "if not has_default:\n",
        "  clf = xgboost.XGBRegressor(tree_method = \"gpu_hist\", gpu_id=0, verbosity=0)\n",
        "  grid_search = GridSearchCV(clf, param_grid={'max_depth':[3]}, cv=5, scoring=\"r2\")\n",
        "  start = time.time()\n",
        "  grid_search.fit(X, y)\n",
        "  print(\"DEFAULT run time: \", (time.time()-start)/60)\n",
        "  default=pd.DataFrame(grid_search.cv_results_)\n",
        "  default['experiment_name']='default params'\n",
        "  default.to_csv(DIR_TO_SAVE_RES+\"res_default_{}.csv\".format(dataset_name),index=False)\n",
        "\n",
        "  print(grid_search.best_score_)\n",
        "# DEFAULT run time:  0.026329147815704345\n",
        "# 0.6726520660378551"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtSyO6FQE77n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# grid:\n",
        "if not has_grid:\n",
        "  param_grid = {'max_depth': [2, 10, 18], 'min_child_weight': [2, 10, 18],\n",
        "              'alpha':[0.25,0.5,0.75], 'lambda':[0.25,0.5,0.75],\n",
        "              'subsample':[0.625, 0.75, 0.875],'colsample_bytree':[0.625, 0.75, 0.875]} \n",
        "\n",
        "  clf = xgboost.XGBRegressor(tree_method = \"gpu_hist\", gpu_id=0, verbosity=0)\n",
        "  grid_search = GridSearchCV(clf, param_grid=param_grid, cv=5, scoring=\"r2\")\n",
        "\n",
        "  start = time.time()\n",
        "  grid_search.fit(X, y)\n",
        "  print(\"GRID run time: \", (time.time()-start)/60)\n",
        "\n",
        "  grid_search_df=pd.DataFrame(grid_search.cv_results_)\n",
        "  grid_search_df['experiment_name']='grid_search'\n",
        "  grid_search_df.to_csv(DIR_TO_SAVE_RES+\"res_grid_search_{}.csv\".format(dataset_name),index=False)\n",
        "  print(grid_search.best_score_)\n",
        "# GRID run time:  27.905986074606577\n",
        "# 0.6828739254131614"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHAIxORtGJ0g",
        "colab_type": "code",
        "outputId": "77321f32-cecc-46b4-f419-ae3c320959d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# random search:\n",
        "if random_search_amount<10:\n",
        "  n_iter_search = 729\n",
        "  param_dist = {\"max_depth\": sp_randint(1, 20),\n",
        "                \"min_child_weight\": sp_randint(1, 20),\n",
        "                \"alpha\": uniform(loc=0, scale=1),\n",
        "                \"lambda\": uniform(loc=0, scale=1),\n",
        "                \"subsample\":uniform(loc=0.5, scale=0.4),\n",
        "                \"colsample_bytree\":uniform(loc=0.5, scale=0.4)}\n",
        "\n",
        "  for i in range(random_search_amount+1, 11):\n",
        "    clf = xgboost.XGBRegressor(tree_method = \"gpu_hist\", gpu_id=0, verbosity=0)\n",
        "    random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
        "                                        n_iter=n_iter_search, cv=5, scoring=\"r2\")\n",
        "    start = time.time()\n",
        "    random_search.fit(X, y) \n",
        "    print(\"RS run time: \", (time.time()-start)/60)\n",
        "    res_rand=pd.DataFrame(random_search.cv_results_)\n",
        "    subset = res_rand.loc[:,['param_max_depth', 'param_min_child_weight', \n",
        "                            'param_alpha','param_lambda',\n",
        "                            'param_colsample_bytree', 'param_subsample']].drop_duplicates().index\n",
        "    res_rand = res_rand.loc[subset]\n",
        "\n",
        "    while res_rand.shape[0]<n_iter_search:\n",
        "      random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
        "                                        n_iter=(n_iter_search-random_search.shape[0]), cv=5, scoring=\"r2\")\n",
        "      random_search.fit(X, y) \n",
        "      res_rand = pd.concat([res_rand, random_search], sort = False)\n",
        "      subset = res_rand.loc[:,['param_max_depth', 'param_min_child_weight', \n",
        "                            'param_alpha','param_lambda',\n",
        "                            'param_colsample_bytree', 'param_subsample']].drop_duplicates().index\n",
        "      res_rand = res_rand.loc[subset]\n",
        "\n",
        "    res_rand['run_number']=i\n",
        "\n",
        "    res_rand['experiment_name']=\"random_search\"\n",
        "    res_rand.to_csv(DIR_TO_SAVE_RES+\"res_rand_{}_{}.csv\".format(dataset_name, i), index=False)\n",
        "\n",
        "\"\"\"\n",
        "RS run time:  27.66507022380829\n",
        "RS run time:  28.061819970607758\n",
        "RS run time:  27.44816317160924\n",
        "RS run time:  27.645976877212526\n",
        "RS run time:  26.519640775521598\n",
        "RS run time:  27.433769524097443\n",
        "RS run time:  28.03506429195404\n",
        "RS run time:  27.815785245100656\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nRS run time:  27.66507022380829\\nRS run time:  28.061819970607758\\nRS run time:  27.44816317160924\\nRS run time:  27.645976877212526\\nRS run time:  26.519640775521598\\nRS run time:  27.433769524097443\\nRS run time:  28.03506429195404\\nRS run time:  27.815785245100656\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRqV0KpjHUm9",
        "colab_type": "code",
        "outputId": "49ed44ee-ee45-4677-96b5-5acc73447cee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "source": [
        "# random group search:\n",
        "if group_random_search_amount<10:\n",
        "  n_iter_search = 243\n",
        "\n",
        "  param_dist_group_1 = {\"max_depth\": sp_randint(1, 20),\n",
        "                    \"min_child_weight\": sp_randint(1, 20)}\n",
        "\n",
        "  param_dist_group_2 = {\"alpha\": uniform(loc=0, scale=1),\n",
        "                    \"lambda\": uniform(loc=0, scale=1)}\n",
        "\n",
        "  param_dist_group_3 = {\"subsample\":uniform(loc=0.5, scale=0.4),\n",
        "                    \"colsample_bytree\":uniform(loc=0.5, scale=0.4)}\n",
        "\n",
        "\n",
        "  for i in range(group_random_search_amount+1, 7):\n",
        "    #first group:\n",
        "    clf = xgboost.XGBRegressor(tree_method = \"gpu_hist\", gpu_id=0, verbosity=0)\n",
        "    random_search = RandomizedSearchCV(clf, param_distributions=param_dist_group_1,\n",
        "                                          n_iter=n_iter_search, cv=5, scoring=\"r2\")\n",
        "\n",
        "    start = time.time()\n",
        "    random_search.fit(X, y)\n",
        "    print(\"RS for first group: \",(time.time() - start)/60)\n",
        "\n",
        "\n",
        "    res_rand_1=pd.DataFrame(random_search.cv_results_)\n",
        "    res_rand_1['experiment_name']='random search for 1 group'      \n",
        "    res_rand_1['run_number']=i\n",
        "\n",
        "    \n",
        "    subset = res_rand_1.loc[:,['param_max_depth', 'param_min_child_weight']].drop_duplicates().index\n",
        "    res_rand_1 = res_rand_1.loc[subset]\n",
        "    print(res_rand_1.shape)\n",
        "    while res_rand_1.shape[0]<n_iter_search:\n",
        "        random_search = RandomizedSearchCV(clf, param_distributions=param_dist_group_1,\n",
        "                                          n_iter=(n_iter_search-res_rand_1.shape[0]), cv=5, scoring=\"r2\")\n",
        "        random_search.fit(X, y) \n",
        "        res_rand_1 = pd.concat([res_rand_1, pd.DataFrame(random_search.cv_results_)], sort = False).reset_index(drop=True)\n",
        "        subset = res_rand_1.loc[:,['param_max_depth', 'param_min_child_weight']].drop_duplicates().index\n",
        "        res_rand_1 = res_rand_1.loc[subset]\n",
        "    print(res_rand_1.shape)\n",
        "    res_rand_1.to_csv(DIR_TO_SAVE_RES+\"res_rand_group1_{}_{}.csv\".format(dataset_name, i), index=False)\n",
        "    max_score = res_rand_1['mean_test_score'].max()\n",
        "    df_max = res_rand_1.loc[res_rand_1['mean_test_score']==max_score]\n",
        "    clf.set_params(**df_max['params'].values[0])\n",
        "\n",
        "    #second group:\n",
        "    random_search = RandomizedSearchCV(clf, param_distributions=param_dist_group_2,\n",
        "                                          n_iter=n_iter_search, cv=5, scoring=\"r2\")\n",
        "\n",
        "    start = time.time()\n",
        "    random_search.fit(X, y)\n",
        "    print(\"RS for second group: \",(time.time() - start)/60)\n",
        "\n",
        "    res_rand_2=pd.DataFrame(random_search.cv_results_)\n",
        "    res_rand_2['experiment_name']='random search for 2 group' \n",
        "    res_rand_2['run_number']=i\n",
        "\n",
        "    res_rand_2.to_csv(DIR_TO_SAVE_RES+\"res_rand_group2_{}_{}.csv\".format(dataset_name, i), index=False)\n",
        "    clf.set_params(**random_search.best_params_)\n",
        "\n",
        "    #third group:\n",
        "    random_search = RandomizedSearchCV(clf, param_distributions=param_dist_group_3,\n",
        "                                          n_iter=n_iter_search, cv=5, scoring=\"r2\")\n",
        "\n",
        "    start = time.time()\n",
        "    random_search.fit(X, y)\n",
        "    print(\"RS for third group: \",(time.time() - start)/60)\n",
        "\n",
        "    res_rand_3=pd.DataFrame(random_search.cv_results_)\n",
        "    res_rand_3['experiment_name']='random search for 3 group' \n",
        "    res_rand_3['run_number']=i\n",
        "    res_rand_3.to_csv(DIR_TO_SAVE_RES+\"res_rand_group3_{}_{}.csv\".format(dataset_name, i), index=False)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RS for first group:  8.946099249521891\n",
            "(184, 17)\n",
            "(243, 17)\n",
            "RS for second group:  5.284519712130229\n",
            "RS for third group:  5.361768352985382\n",
            "RS for first group:  9.198115090529123\n",
            "(171, 17)\n",
            "(243, 17)\n",
            "RS for second group:  5.282854831218719\n",
            "RS for third group:  5.360952051480611\n",
            "RS for first group:  8.984747664133709\n",
            "(182, 17)\n",
            "(243, 17)\n",
            "RS for second group:  5.282317062218984\n",
            "RS for third group:  5.361859313646952\n",
            "RS for first group:  8.706163032849629\n",
            "(179, 17)\n",
            "(243, 17)\n",
            "RS for second group:  5.28796079158783\n",
            "RS for third group:  5.366481220722198\n",
            "RS for first group:  9.23592178026835\n",
            "(179, 17)\n",
            "(243, 17)\n",
            "RS for second group:  5.2916605512301125\n",
            "RS for third group:  5.37176075776418\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-a6f241736805>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RS for first group: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[1;32m   1483\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m             random_state=self.random_state))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 253\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 253\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    394\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7KtKjY1_RfL",
        "colab_type": "text"
      },
      "source": [
        "# SECOND TEST: STOP CRITERION FOR RANDOM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hto6Lg-g_YcT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def group_best_params_with_stop(df, stop_iter_num, left_iter_num, right_iter_num):\n",
        "    best_params = {}\n",
        "    stop_iter_nums = {}\n",
        "    best_score = {}\n",
        "\n",
        "    tmp_df = df.loc[(df['iter_num'] >= left_iter_num) & (df['iter_num'] <= right_iter_num)].copy() #take group\n",
        "    for i in tmp_df.run_number.unique(): \n",
        "      tmp_df_i = tmp_df.loc[tmp_df.run_number==i] #take run\n",
        "      #cut iterations:\n",
        "      tmp_df_i_copy = tmp_df_i.copy()\n",
        "      tmp_df_i_copy.loc[:, \"mean_test_score\"] = tmp_df_i.loc[:, \"mean_test_score\"].cummax()      \n",
        "\n",
        "      try:\n",
        "          curr_max=tmp_df_i_copy.groupby(\"mean_test_score\").count().reset_index().sort_values(by = \"mean_test_score\")\n",
        "          max_score = curr_max.loc[curr_max[\"mean_test_score\"]>=stop_iter_num].index[0] #get first interval with iter_amount more than stop_iter_num\n",
        "      except:\n",
        "          max_score = tmp_df_i_copy.mean_test_score.max()\n",
        "      \n",
        "      tmp_stop = tmp_df_i.loc[tmp_df_i[\"mean_test_score\"]==max_score].sort_values(by=\"iter_num\").iloc[0]#get first element with best_score\n",
        "\n",
        "      best_params[i] = tmp_stop['params']\n",
        "      #best_score[i] = tmp_stop['mean_test_score']\n",
        "\n",
        "      last_iter_num = tmp_stop['iter_num'] + stop_iter_num - 1\n",
        "      if last_iter_num > right_iter_num:\n",
        "          last_iter_num = right_iter_num\n",
        "\n",
        "      stop_iter_nums[i] = last_iter_num\n",
        "    return best_params, stop_iter_nums#, best_score       \n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28xz465CEC4k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "59deca38-194d-427f-f6b3-6fac47a241e2"
      },
      "source": [
        "#for params_set_num in best_params_first_group:\n",
        "if first_group_stop_params:\n",
        "  param_dist_group_2 = {\"alpha\": uniform(loc=0, scale=1),\n",
        "                  \"lambda\": uniform(loc=0, scale=1)}\n",
        "\n",
        "  param_dist_group_3 = {\"subsample\":uniform(loc=0.5, scale=0.4),\n",
        "                    \"colsample_bytree\":uniform(loc=0.5, scale=0.4)}\n",
        "  for params_set_num in range(1,11):\n",
        "    clf = xgboost.XGBRegressor(tree_method = \"gpu_hist\", gpu_id=0, verbosity=0)\n",
        "    clf.set_params(**first_group_stop_params[params_set_num])\n",
        "    n_iter_search_sec = int(round((729 - first_group_stop_iters[params_set_num])/2))\n",
        "    #second group:\n",
        "    random_search = RandomizedSearchCV(clf, param_distributions=param_dist_group_2,\n",
        "                                        n_iter=n_iter_search_sec, cv=5, scoring=\"r2\")\n",
        "\n",
        "    random_search.fit(X, y)\n",
        "\n",
        "    res_rand_2=pd.DataFrame(random_search.cv_results_)\n",
        "    res_rand_2['iter_num'] = range(first_group_stop_iters[params_set_num]+1, \n",
        "                                  first_group_stop_iters[params_set_num]+n_iter_search_sec+1)\n",
        "    res_rand_2['experiment_name']='random search for 2 group' \n",
        "    res_rand_2['run_number']=params_set_num\n",
        "    res_rand_2.to_csv(DIR_TO_SAVE_RES+'GR_S_2_group_with_{}_{}_{}.csv'.format(stop_criterion, params_set_num, dataset_name), index=False)\n",
        "    #fit last suitable params\n",
        "    best_params_second_group, sec_stop_iter_nums = group_best_params_with_stop(res_rand_2, \n",
        "                                                          stop_iter_num = stop_criterion, \n",
        "                                                          left_iter_num=first_group_stop_iters[params_set_num]+1, \n",
        "                                                          right_iter_num=first_group_stop_iters[params_set_num]+n_iter_search_sec+1)\n",
        "    clf.set_params(**best_params_second_group[params_set_num])\n",
        "    \n",
        "    print('done for second group')\n",
        "\n",
        "    #third group:\n",
        "    n_iter_search_third = 729 - sec_stop_iter_nums[params_set_num]\n",
        "    random_search = RandomizedSearchCV(clf, param_distributions=param_dist_group_3,\n",
        "                                          n_iter=n_iter_search_third, cv=5, scoring=\"r2\")\n",
        "    random_search.fit(X, y)\n",
        "\n",
        "    res_rand_3=pd.DataFrame(random_search.cv_results_)\n",
        "    res_rand_3['iter_num'] = range(sec_stop_iter_nums[params_set_num]+1, 729+1)\n",
        "    res_rand_3['experiment_name']='random search for 3 group' \n",
        "    res_rand_3['run_number']=params_set_num\n",
        "\n",
        "    best_params_third_group, third_stop_iter_nums = group_best_params_with_stop(res_rand_3, \n",
        "                                                          stop_iter_num = stop_criterion, \n",
        "                                                          left_iter_num=sec_stop_iter_nums[params_set_num]+1, \n",
        "                                                          right_iter_num=729)\n",
        "    res_rand_3=res_rand_3.loc[(res_rand_3['run_number']==params_set_num) & \\\n",
        "                            (res_rand_3['iter_num']<=int(third_stop_iter_nums[params_set_num]))]\n",
        "\n",
        "    res=pd.concat([res_rand_2,res_rand_3], sort=False)\n",
        "\n",
        "    sec_idxs = res.loc[res['experiment_name']=='random search for 2 group','iter_num'].values\n",
        "    third_idxs = res.loc[res['experiment_name']=='random search for 3 group','iter_num'].values\n",
        "    intersection = np.intersect1d(third_idxs, sec_idxs)\n",
        "\n",
        "    res = res.loc[~((res['experiment_name']=='random search for 2 group') & (res['iter_num'].isin(intersection)))]\n",
        "\n",
        "    res.to_csv(DIR_TO_SAVE_RES+'GR_S_2_and_3_groups_with_{}_{}_{}.csv'.format(stop_criterion, params_set_num, dataset_name), index=False)\n",
        "    print(first_group_stop_iters[params_set_num]+res.shape[0])\n",
        "    print('done for third group')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done for second group\n",
            "661\n",
            "done for third group\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egsRBPWd_ZM2",
        "colab_type": "text"
      },
      "source": [
        "# THIRD TEST: BAYES AND GROUPED BAYES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTHl4mIZ_eHr",
        "colab_type": "text"
      },
      "source": [
        "# FOURTH TEST: STOP CRITERION FOR BAYES"
      ]
    }
  ]
}