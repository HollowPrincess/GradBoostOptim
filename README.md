# GradBoostOptim
Distributed optimization of hyperparameters of gradient boosting algorithms

## Описание
Данный репозиторий содержит ресурсы, полученные в результате научной работы на тему "Распределенная оптимизация гиперпараметров алгоритмов градиентного бустинга"

Главный каталог содержит три подкаталога:
- **data** - директория, содержащая тестовый датасет и данные, полученные в ходе работы
- **graphs** - директория, которая сдержит графики, полученные в ходе проведения экспериментов
- **test 1** - директория, содержащая .ipynb файлы, использованные для проведения первого эксперимента, ход и описание которого содержатся в pdf файле на [диске](https://drive.google.com/file/d/1P81hWve80FIj2JRjVHmDaDiv85Va4k2v/view)

Папки **data** и **graphs** также содержат папку **test 1**.

## Первый эксперимент
__Цель__: Обосновать правомощность использования предлагаемого способа оптимизации гиперпараметров XGBoost при использовании случайного поиска.

Для достижения поставленной цели был проведен эксперимент, в рамках которого были сравнены результаты работы трех методов оптимизации: поиск по сетке, случайный поиск и поиск с разбиением на группы. Сравнение проводилось на наборе данных Facebook Comment Volume Dataset.

Был произведен сопутствующий анализ данных, в ходе которого было выявлено, что поиск с разбиением показал себя не хуже, чем случайный поиск, а также были сделаны интересные выводы, связанные с гиперпараметрами, временем работы и числом итераций, необходимым для достижения оптимума.

## Второй эксперимент
__Цель__: Сравнение возможностей сокращения вычислений за счет введения критерия остановки по числу
итераций без изменения максимально достигнутой оценки, а также проверка наличия взаимосвязи между критерием остановки и итоговой оценкой.

Для этого был проведен эксперимент, в рамках которого были сравнены результаты работы случайного поиска и случайного поиска с разбиением на группы с использованием различных критериев остановки по числу итераций без изменения максимально достигнутой оценки качества. В качестве значений критерия были рассмотрены значения 100, 50, 25 и 10.

Был произведен сопутствующий анализ данных, в ходе которого было выявлено, что поиск с разбиением на группы ведет себя устойчиво при введении критерия остановки: етод сходится к тем же значениям, что и без использования критерия, при этом удается сократить число итераций метода оптимизации.

## Третий эксперимент
__Цель__: Обосновать правомощность использования предлагаемого способа оптимизации гиперпараметров XGBoost при использовании байесовской оптимизации.

Этот эксперимент точно такой же, как и первый эксперимент, только помимо случайного поиска рассматривается еще и байесовская оптмизация.
Работа над этим экспериментом еще ведется, так как возникли проблемы с выбором фреймворка для байесовской оптимизации.


